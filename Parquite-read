code dependancy : -
<dependencies>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-aws</artifactId>
        <version>3.2.2</version>
    </dependency>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-common</artifactId>
        <version>3.2.2</version>
    </dependency>
    <dependency>
        <groupId>org.apache.parquet</groupId>
        <artifactId>parquet-hadoop</artifactId>
        <version>1.11.1</version>
    </dependency>
    <dependency>
        <groupId>com.amazonaws</groupId>
        <artifactId>aws-java-sdk-bundle</artifactId>
        <version>1.11.969</version>
    </dependency>
</dependencies>

2)Configure Hadoop to connect to S3
import org.apache.hadoop.conf.Configuration;

public class S3Configuration {
    public static Configuration getHadoopConfiguration() {
        Configuration conf = new Configuration();
        conf.set("fs.s3a.access.key", "your-access-key");
        conf.set("fs.s3a.secret.key", "your-secret-key");
        conf.set("fs.s3a.endpoint", "s3.amazonaws.com");
        conf.set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem");
        return conf;
    }
}


import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.parquet.hadoop.ParquetFileReader;
import org.apache.parquet.hadoop.metadata.ParquetMetadata;
import org.apache.parquet.schema.MessageType;
import org.apache.parquet.tools.read.SimpleReadSupport;

import java.io.IOException;

public class ParquetReaderExample {
    public static void main(String[] args) {
        Configuration conf = S3Configuration.getHadoopConfiguration();
        Path parquetFilePath = new Path("s3a://your-bucket/your-parquet-file.parquet");

        try {
            ParquetMetadata readFooter = ParquetFileReader.readFooter(conf, parquetFilePath);
            MessageType schema = readFooter.getFileMetaData().getSchema();

            System.out.println("Parquet schema: " + schema);

            ParquetFileReader reader = ParquetFileReader.open(conf, parquetFilePath, new SimpleReadSupport());
            // Read and process the Parquet file content here
            reader.close();
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
}
logic started

<dependencies>
    <dependency>
        <groupId>software.amazon.awssdk</groupId>
        <artifactId>s3</artifactId>
        <version>2.17.106</version>
    </dependency>
    <dependency>
        <groupId>org.apache.parquet</groupId>
        <artifactId>parquet-avro</artifactId>
        <version>1.12.2</version>
    </dependency>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-client</artifactId>
        <version>3.3.1</version>
    </dependency>
</dependencies>

import software.amazon.awssdk.auth.credentials.ProfileCredentialsProvider;
import software.amazon.awssdk.regions.Region;
import software.amazon.awssdk.services.s3.S3Client;
import software.amazon.awssdk.services.s3.model.GetObjectRequest;
import software.amazon.awssdk.services.s3.model.S3Object;

import java.nio.file.Paths;

public class S3ParquetReader {
    private static final Region REGION = Region.US_WEST_2;
    private static final String BUCKET_NAME = "your-bucket-name";
    private static final String KEY = "path/to/your-file.parquet";

    public static void main(String[] args) {
        S3Client s3 = S3Client.builder()
                .region(REGION)
                .credentialsProvider(ProfileCredentialsProvider.create())
                .build();

        GetObjectRequest getObjectRequest = GetObjectRequest.builder()
                .bucket(BUCKET_NAME)
                .key(KEY)
                .build();

        s3.getObject(getObjectRequest, Paths.get("/path/to/local/parquet-file.parquet"));
    }
}

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.parquet.hadoop.ParquetReader;
import org.apache.parquet.hadoop.metadata.CompressionCodecName;
import org.apache.parquet.hadoop.util.HadoopInputFile;
import org.apache.parquet.avro.AvroParquetReader;
import org.apache.avro.generic.GenericRecord;

import java.io.IOException;

public class ParquetFileReader {
    public static void main(String[] args) {
        Path parquetFilePath = new Path("/path/to/local/parquet-file.parquet");
        Configuration configuration = new Configuration();

        try (ParquetReader<GenericRecord> reader = AvroParquetReader.<GenericRecord>builder(HadoopInputFile.fromPath(parquetFilePath, configuration)).build()) {
            GenericRecord record;
            while ((record = reader.read()) != null) {
                System.out.println(record);
            }
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
}

import software.amazon.awssdk.auth.credentials.ProfileCredentialsProvider;
import software.amazon.awssdk.regions.Region;
import software.amazon.awssdk.services.s3.S3Client;
import software.amazon.awssdk.services.s3.model.GetObjectRequest;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.parquet.hadoop.ParquetReader;
import org.apache.parquet.avro.AvroParquetReader;
import org.apache.avro.generic.GenericRecord;
import java.nio.file.Paths;
import java.io.IOException;

public class S3ParquetReader {
    private static final Region REGION = Region.US_WEST_2;
    private static final String BUCKET_NAME = "your-bucket-name";
    private static final String KEY = "path/to/your-file.parquet";

    public static void main(String[] args) {
        // Step 1: Initialize AWS S3 Client
        S3Client s3 = S3Client.builder()
                .region(REGION)
                .credentialsProvider(ProfileCredentialsProvider.create())
                .build();

        // Step 2: Download Parquet file from S3 to local filesystem
        GetObjectRequest getObjectRequest = GetObjectRequest.builder()
                .bucket(BUCKET_NAME)
                .key(KEY)
                .build();
        s3.getObject(getObjectRequest, Paths.get("/path/to/local/parquet-file.parquet"));

        // Step 3: Read the Parquet file
        Path parquetFilePath = new Path("/path/to/local/parquet-file.parquet");
        Configuration configuration = new Configuration();

        try (ParquetReader<GenericRecord> reader = AvroParquetReader.<GenericRecord>builder(parquetFilePath).withConf(configuration).build()) {
            GenericRecord record;
            while ((record = reader.read()) != null) {
                System.out.println(record);
            }
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
}

single java code

import com.amazonaws.auth.AWSStaticCredentialsProvider;
import com.amazonaws.auth.BasicAWSCredentials;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.S3Object;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.fs.s3a.S3AFileSystem;
import org.apache.parquet.hadoop.ParquetFileReader;
import org.apache.parquet.hadoop.metadata.ParquetMetadata;
import org.apache.parquet.schema.MessageType;

import java.io.InputStream;

public class S3ParquetReader {
    public static void main(String[] args) throws Exception {
        String bucketName = "your-bucket-name";
        String key = "path/to/your/file.parquet";
        String region = "your-region"; // e.g., "us-west-2"
        String accessKey = "your-access-key";
        String secretKey = "your-secret-key";

        // Initialize AWS S3 client
        BasicAWSCredentials awsCreds = new BasicAWSCredentials(accessKey, secretKey);
        AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                .withRegion(region)
                .withCredentials(new AWSStaticCredentialsProvider(awsCreds))
                .build();

        // Get the Parquet file from S3
        S3Object s3Object = s3Client.getObject(bucketName, key);
        InputStream inputStream = s3Object.getObjectContent();

        // Set up Hadoop configuration to use S3
        Configuration conf = new Configuration();
        conf.set("fs.s3a.access.key", accessKey);
        conf.set("fs.s3a.secret.key", secretKey);
        conf.set("fs.s3a.endpoint", "s3." + region + ".amazonaws.com");
        conf.set("fs.s3a.impl", S3AFileSystem.class.getName());

        // Read the Parquet file
        Path path = new Path("s3a://" + bucketName + "/" + key);
        ParquetMetadata readFooter = ParquetFileReader.readFooter(conf, path);
        MessageType schema = readFooter.getFileMetaData().getSchema();

        // Print the schema
        System.out.println("Schema: " + schema);
    }
}

new code start
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.parquet.example.data.Group;
import org.apache.parquet.example.data.simple.SimpleGroup;
import org.apache.parquet.hadoop.ParquetFileReader;
import org.apache.parquet.hadoop.metadata.ParquetMetadata;
import org.apache.parquet.schema.MessageType;

import java.net.URI;

public class ParquetReaderExample {
    public static void main(String[] args) {
        try {
            Configuration conf = new Configuration();
            // Set AWS credentials (if needed, depending on your setup)
            conf.set("fs.s3a.access.key", "YOUR_AWS_ACCESS_KEY");
            conf.set("fs.s3a.secret.key", "YOUR_AWS_SECRET_KEY");
            // Set the S3 file path
            String s3FilePath = "s3a://your-bucket/your-file.parquet";
            Path parquetFilePath = new Path(new URI(s3FilePath));

            // Open Parquet file reader
            FileSystem fs = parquetFilePath.getFileSystem(conf);
            ParquetMetadata readFooter = ParquetFileReader.readFooter(conf, parquetFilePath);
            MessageType schema = readFooter.getFileMetaData().getSchema();

            try (ParquetFileReader reader = ParquetFileReader.open(conf, parquetFilePath)) {
                // Read all rows
                SimpleGroup record;
                while ((record = reader.read()) != null) {
                    // Process each record
                    System.out.println(record);
                }
            }

        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}
<dependencies>
    <dependency>
        <groupId>org.apache.parquet</groupId>
        <artifactId>parquet-avro</artifactId>
        <version>1.12.0</version>
    </dependency>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-common</artifactId>
        <version>3.2.1</version>
    </dependency>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-client</artifactId>
        <version>3.2.1</version>
    </dependency>
</dependencies>

logic code new 
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.fs.s3a.S3AFileSystem;
import org.apache.parquet.hadoop.ParquetReader;
import org.apache.parquet.hadoop.util.HadoopInputFile;
import org.apache.parquet.schema.MessageType;

public class ParquetReaderExample {
    public static void main(String[] args) {
        String bucketName = "your-bucket-name";
        String key = "path/to/your-file.parquet";
        
        Configuration conf = new Configuration();
        conf.set("fs.s3a.access.key", "YOUR_ACCESS_KEY");
        conf.set("fs.s3a.secret.key", "YOUR_SECRET_KEY");
        conf.set("fs.s3a.endpoint", "s3.amazonaws.com");

        try {
            Path parquetPath = new Path("s3a://" + bucketName + "/" + key);
            HadoopInputFile inputFile = HadoopInputFile.fromPath(parquetPath, conf);
            ParquetReader<GenericRecord> reader = AvroParquetReader.<GenericRecord>builder(inputFile).build();

            GenericRecord record;
            while ((record = reader.read()) != null) {
                System.out.println(record);
            }

            reader.close();
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}

<dependency>
    <groupId>org.apache.parquet</groupId>
    <artifactId>parquet-avro</artifactId>
    <version>1.12.3</version>
</dependency>
<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-common</artifactId>
    <version>3.3.0</version>
</dependency>
<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-client</artifactId>
    <version>3.3.0</version>
</dependency>

<dependency>
    <groupId>com.amazonaws</groupId>
    <artifactId>aws-java-sdk-s3</artifactId>
    <version>1.12.100</version>
</dependency>

