Step 1: Set Up Your Spring Boot Project

Step 2: Add Dependencies
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-web</artifactId>
</dependency>
<dependency>
    <groupId>com.amazonaws</groupId>
    <artifactId>aws-java-sdk-s3</artifactId>
    <version>1.12.569</version>
</dependency>
<dependency>
    <groupId>org.apache.parquet</groupId>
    <artifactId>parquet-hadoop</artifactId>
    <version>1.12.3</version>
</dependency>
<dependency>
    <groupId>org.apache.parquet</groupId>
    <artifactId>parquet-avro</artifactId>
    <version>1.12.3</version>
</dependency>
<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-common</artifactId>
    <version>3.3.4</version>
</dependency>
<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-client</artifactId>
    <version>3.3.4</version>
</dependency>

Step 3: Configure AWS Credentials
You can set up your AWS credentials in your application.properties file or use environment variables. Hereâ€™s an example using application.properties:
aws.accessKeyId=your-access-key-id
aws.secretKey=your-secret-key
aws.region=your-region
aws.s3.bucket=your-bucket-name

Step 4: Implement a Service to Read the Parquet File from S3
Create a service to handle the logic for reading the Parquet file from S3:
import com.amazonaws.auth.AWSStaticCredentialsProvider;
import com.amazonaws.auth.BasicAWSCredentials;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.S3Object;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.fs.s3a.S3AFileSystem;
import org.apache.parquet.hadoop.ParquetReader;
import org.apache.parquet.hadoop.example.GroupReadSupport;
import org.apache.parquet.hadoop.util.HadoopInputFile;
import org.apache.parquet.example.data.Group;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Service;

import javax.annotation.PostConstruct;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

@Service
public class ParquetService {

    @Value("${aws.accessKeyId}")
    private String accessKeyId;

    @Value("${aws.secretKey}")
    private String secretKey;

    @Value("${aws.region}")
    private String region;

    @Value("${aws.s3.bucket}")
    private String bucketName;

    private AmazonS3 s3Client;

    @PostConstruct
    private void initializeAmazon() {
        BasicAWSCredentials awsCreds = new BasicAWSCredentials(accessKeyId, secretKey);
        this.s3Client = AmazonS3ClientBuilder.standard()
                .withRegion(Regions.fromName(region))
                .withCredentials(new AWSStaticCredentialsProvider(awsCreds))
                .build();
    }

    public List<Group> readParquetFile(String fileKey) throws IOException {
        S3Object s3Object = s3Client.getObject(bucketName, fileKey);

        Configuration hadoopConfig = new Configuration();
        hadoopConfig.set("fs.s3a.access.key", accessKeyId);
        hadoopConfig.set("fs.s3a.secret.key", secretKey);
        hadoopConfig.set("fs.s3a.endpoint", "s3.amazonaws.com");

        Path path = new Path("s3a://" + bucketName + "/" + fileKey);
        S3AFileSystem s3aFileSystem = new S3AFileSystem();
        s3aFileSystem.initialize(path.toUri(), hadoopConfig);

        List<Group> records = new ArrayList<>();
        try (FSDataInputStream inputStream = s3aFileSystem.open(path)) {
            ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), HadoopInputFile.fromStream(inputStream, hadoopConfig))
                    .build();

            Group group;
            while ((group = reader.read()) != null) {
                records.add(group);
            }
        }
        return records;
    }
}

Step 5: Create a REST Controller
Create a REST controller to handle the GET request and return the Parquet data as JSON:
import org.apache.parquet.example.data.Group;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestParam;
import org.springframework.web.bind.annotation.RestController;

import java.io.IOException;
import java.util.List;
import java.util.stream.Collectors;

@RestController
public class ParquetController {

    @Autowired
    private ParquetService parquetService;

    @GetMapping("/read-parquet")
    public List<String> readParquetFile(@RequestParam String fileKey) throws IOException {
        List<Group> records = parquetService.readParquetFile(fileKey);
        return records.stream()
                .map(Group::toString)
                .collect(Collectors.toList());
    }
}


ex: simple parquite file read
<dependencies>
    <dependency>
        <groupId>org.apache.parquet</groupId>
        <artifactId>parquet-hadoop</artifactId>
        <version>1.12.3</version>
    </dependency>
    <dependency>
        <groupId>org.apache.parquet</groupId>
        <artifactId>parquet-avro</artifactId>
        <version>1.12.3</version>
    </dependency>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-common</artifactId>
        <version>3.3.4</version>
    </dependency>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-client</artifactId>
        <version>3.3.4</version>
    </dependency>
</dependencies>

step 2

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.parquet.avro.AvroParquetReader;
import org.apache.parquet.hadoop.ParquetReader;
import org.apache.avro.generic.GenericRecord;

public class ParquetReaderExample {

    public static void main(String[] args) {
        String parquetFilePath = "path/to/your/file.parquet"; // Replace with your Parquet file path

        Configuration configuration = new Configuration();
        Path path = new Path(parquetFilePath);

        try (ParquetReader<GenericRecord> reader = AvroParquetReader.<GenericRecord>builder(path)
                .withConf(configuration)
                .build()) {

            GenericRecord record;
            while ((record = reader.read()) != null) {
                System.out.println(record);
            }

        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}

new ex of s3 keys 
step 1 : controller
import org.apache.hadoop.fs.Path;
import org.apache.parquet.avro.AvroParquetReader;
import org.apache.parquet.hadoop.ParquetReader;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestParam;
import org.springframework.web.bind.annotation.RestController;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

@RestController
public class ParquetApiController {

    @Value("${s3.bucket.name}")
    private String bucketName;  // Replace with your bucket name

    @GetMapping("/readParquet")
    public ResponseEntity<List<Object>> readParquetFile(@RequestParam String s3Key) {
        List<Object> data = new ArrayList<>();

        try {
            String s3Path = "s3://" + bucketName + "/" + s3Key;
            Path path = new Path(s3Path);

            ParquetReader<Object> reader = AvroParquetReader.<Object>builder(path).build();

            Object record;
            while ((record = reader.read()) != null) {
                data.add(record);
            }

            reader.close();
        } catch (IOException e) {
            // Handle exception appropriately
            e.printStackTrace();
            return ResponseEntity.status(500).body(null);
        }

        return ResponseEntity.ok(data);
    }
}

step 2 : application property 
# AWS Credentials
aws.access.key=<your-access-key>
aws.secret.key=<your-secret-key>

# S3 Bucket Name
s3.bucket.name=<your-s3-bucket-name>

New new ex
step 1: Dependancy

<dependencies>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-web</artifactId>
    </dependency>
    <dependency>
        <groupId>software.amazon.awssdk</groupId>
        <artifactId>s3</artifactId>
    </dependency>
    <dependency>
        <groupId>org.apache.parquet</groupId>
        <artifactId>parquet-avro</artifactId>
        <version>1.11.1</version>
    </dependency>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-common</artifactId>
        <version>3.3.1</version>
    </dependency>
</dependencies>

step 2:
services class
package com.example.demo.service;

import org.apache.avro.generic.GenericData;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.parquet.avro.AvroParquetReader;
import org.apache.parquet.hadoop.ParquetReader;
import org.springframework.stereotype.Service;
import software.amazon.awssdk.auth.credentials.ProfileCredentialsProvider;
import software.amazon.awssdk.core.sync.RequestBody;
import software.amazon.awssdk.regions.Region;
import software.amazon.awssdk.services.s3.S3Client;
import software.amazon.awssdk.services.s3.model.GetObjectRequest;

import java.io.ByteArrayOutputStream;
import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Paths;

@Service
public class S3ParquetService {

    private final S3Client s3Client;

    public S3ParquetService() {
        this.s3Client = S3Client.builder()
                .region(Region.US_EAST_1)
                .credentialsProvider(ProfileCredentialsProvider.create())
                .build();
    }

    public String readParquetFile(String bucketName, String s3Key) throws IOException {
        // Download the parquet file from S3
        GetObjectRequest getObjectRequest = GetObjectRequest.builder()
                .bucket(bucketName)
                .key(s3Key)
                .build();

        ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
        s3Client.getObject(getObjectRequest, outputStream);

        // Write the downloaded file to a temporary file
        java.nio.file.Path tempFile = Files.createTempFile("temp", ".parquet");
        Files.write(tempFile, outputStream.toByteArray());

        // Read the Parquet file and convert to JSON
        Configuration conf = new Configuration();
        Path path = new Path(tempFile.toUri());
        ParquetReader<GenericData.Record> reader = AvroParquetReader.<GenericData.Record>builder(path).withConf(conf).build();

        StringBuilder jsonResult = new StringBuilder("[");
        GenericData.Record record;
        while ((record = reader.read()) != null) {
            jsonResult.append(record.toString()).append(",");
        }
        reader.close();
        Files.delete(tempFile);

        if (jsonResult.length() > 1) {
            jsonResult.setLength(jsonResult.length() - 1); // remove the last comma
        }
        jsonResult.append("]");

        return jsonResult.toString();
    }
}

step 3 :controller

package com.example.demo.controller;

import com.example.demo.service.S3ParquetService;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestParam;
import org.springframework.web.bind.annotation.RestController;

import java.io.IOException;

@RestController
public class ParquetController {

    @Autowired
    private S3ParquetService s3ParquetService;

    @GetMapping("/read-parquet")
    public String readParquet(@RequestParam String bucketName, @RequestParam String s3Key) {
        try {
            return s3ParquetService.readParquetFile(bucketName, s3Key);
        } catch (IOException e) {
            return "Error reading Parquet file: " + e.getMessage();
        }
    }
}

